import os
import json
import logging
from flask import Flask, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA, LLMChain
from langchain.schema import Document

# üì¶ Cache en memoria
correccion_cache = {}
CACHE_FILE = "cache.json"
PALABRAS_SENSIBLES_FILE = "palabras_sensibles.txt"

# üìù Configurar logging
logging.basicConfig(
    filename="app.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    encoding="utf-8"
)

# üõ°Ô∏è Cargar palabras sensibles desde archivo externo
def cargar_palabras_sensibles():
    if not os.path.exists(PALABRAS_SENSIBLES_FILE):
        mensaje = "‚ö†Ô∏è Advertencia: No se encontr√≥ 'palabras_sensibles.txt'. No se aplicar√° el filtro de protecci√≥n."
        print(mensaje)
        logging.warning(mensaje)
        return []

    with open(PALABRAS_SENSIBLES_FILE, "r", encoding="utf-8") as f:
        lineas = [line.strip().lower() for line in f.readlines() if line.strip()]

    if not lineas:
        mensaje = "‚ö†Ô∏è Advertencia: 'palabras_sensibles.txt' est√° vac√≠o. El filtro de protecci√≥n no detectar√° frases sensibles."
        print(mensaje)
        logging.warning(mensaje)
    else:
        logging.info(f"‚úÖ Se cargaron {len(lineas)} palabras sensibles desde '{PALABRAS_SENSIBLES_FILE}'.")

    return lineas

PALABRAS_SENSIBLES = cargar_palabras_sensibles()

# üì¶ Funciones de cache

def cargar_cache():
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def guardar_cache():
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(correccion_cache, f, indent=2, ensure_ascii=False)

# üå± Cargar entorno
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("‚ùå No se encontr√≥ la API Key. Revisa tu archivo .env.")

# üöÄ Inicializar Flask
app = Flask(__name__)
CORS(app)

# üß† Configurar embeddings y modelos GPT
embedding = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key=openai_api_key
)

llm = ChatOpenAI(model="gpt-4o", openai_api_key=openai_api_key, temperature=0)
corrector_llm = ChatOpenAI(model="gpt-4o", openai_api_key=openai_api_key, temperature=0)
parafrasis_llm = ChatOpenAI(model="gpt-4o", openai_api_key=openai_api_key, temperature=0)

# üì¶ FAISS din√°mico por especialidad
vectordbs = {}

# üìú Prompt base
prompt_template = """
Est√°s actuando como un asistente especializado en el tema de {especialidad_formateada}, enfocado en resolver preguntas de estudiantes de ciencias de la salud sobre este tema. 
Tu conocimiento se basa exclusivamente en libros, decretos, resoluciones y gu√≠as cl√≠nicas oficiales previamente cargadas al sistema.

‚ö†Ô∏è Este chat est√° dedicado al tema de: {especialidad_formateada}.

Tu tarea es responder exclusivamente bas√°ndote en el siguiente contexto proporcionado. 
No debes inventar informaci√≥n ni extrapolar m√°s all√° de lo que est√© expl√≠citamente respaldado en los documentos proporcionados.

Si es posible, incluye citas textuales o referencias directas del contenido para respaldar tus respuestas. 
Las citas deben ir entre comillas ("").

Formato de respuesta:
Respuesta:
Contexto: [Resumen del contenido m√°s relevante]
Justificaci√≥n: [Explicaci√≥n o referencia basada en el contenido del documento, incluyendo citas textuales si corresponde]

Contexto:
{context}

Pregunta:
{question}
"""

# üßπ Correcci√≥n ortogr√°fica + filtro sensible
def corregir_ortografia_gpt(texto):
    texto_limpio = texto.lower().strip()

    if any(palabra in texto_limpio for palabra in PALABRAS_SENSIBLES):
        mensaje = "‚ö†Ô∏è Pregunta sensible detectada. Derivando a mensaje de apoyo."
        print(mensaje)
        logging.warning(mensaje)
        return (
            "Si est√°s atravesando un momento dif√≠cil, es muy importante que hables con alguien de confianza "
            "o busques apoyo profesional. No est√°s solo/a. Comun√≠cate con una l√≠nea de ayuda en tu pa√≠s o acude a un profesional de salud mental."
        )

    if texto in correccion_cache:
        print("‚ö° Recuperando correcci√≥n de cache (memoria/disco).")
        return correccion_cache[texto]

    prompt_correccion = PromptTemplate(
        input_variables=["texto"],
        template=(
            "Corrige √∫nicamente errores ortogr√°ficos o gramaticales en el siguiente texto en espa√±ol. "
            "No agregues consejos, emociones ni explicaciones. Devuelve el texto corregido, o igual si no hay errores:\n\n"
            "Texto: {texto}\n\nTexto corregido:"
        )
    )

    chain = LLMChain(llm=corrector_llm, prompt=prompt_correccion)
    respuesta = chain.run({"texto": texto}).strip()

    correccion_cache[texto] = respuesta
    guardar_cache()
    print("üíæ Correcci√≥n nueva almacenada en cache y guardada en disco.")
    return respuesta

# üîÅ Par√°frasis inteligente con GPT
def parafrasear_pregunta(texto, tema):
    prompt = PromptTemplate(
        input_variables=["pregunta", "tema"],
        template=(
            "Est√°s reformulando una pregunta de un estudiante sobre el tema de {tema}. "
            "Tu objetivo es que la pregunta sea m√°s clara, directa y espec√≠fica, "
            "pero sin a√±adir informaci√≥n nueva ni extender su contenido. "
            "Solo mejora la redacci√≥n si es necesario. "
            "Si ya es clara, rep√≠tela igual.\n\n"
            "Pregunta original: {pregunta}\n\nPregunta reformulada:"
        )
    )
    chain = LLMChain(llm=parafrasis_llm, prompt=prompt)
    return chain.run({"pregunta": texto, "tema": tema}).strip()

# üîÑ RAG Fusion - combinar resultados de reformulaciones
def recuperar_fragmentos_fusionados(pregunta, retriever):
    reformulaciones = [
        pregunta,
        f"¬øQu√© significa {pregunta} en el contexto de salud p√∫blica?",
        f"¬øCu√°l es el prop√≥sito o funci√≥n de {pregunta}?",
        f"Explica el rol de {pregunta} en el sistema de salud."
    ]

    documentos = []
    for reformulada in reformulaciones:
        docs = retriever.get_relevant_documents(reformulada)
        documentos.extend(docs)

    # Eliminar duplicados conservando orden
    vistos = set()
    docs_unicos = []
    for doc in documentos:
        if doc.page_content not in vistos:
            vistos.add(doc.page_content)
            docs_unicos.append(doc)

    return docs_unicos

# üö™ Endpoint din√°mico
@app.route("/chat/<especialidad>", methods=["POST"])
def chat_especialidad(especialidad):
    data = request.get_json()
    question = data.get("question", "")

    if not question:
        return jsonify({"error": "Falta la pregunta"}), 400

    try:
        question_corregida = corregir_ortografia_gpt(question)
        especialidad_formateada = especialidad.replace("_", " ").capitalize()
        pregunta_final = parafrasear_pregunta(question_corregida, especialidad_formateada)

        print(f"üîµ Especialidad solicitada: {especialidad}")
        print(f"üîµ Pregunta original: {question}")
        print(f"üîµ Pregunta corregida: {question_corregida}")
        print(f"üîµ Pregunta reformulada: {pregunta_final}")

        if especialidad not in vectordbs:
            faiss_path = f"data/{especialidad}"
            if not os.path.exists(faiss_path):
                return jsonify({"error": f"La especialidad '{especialidad}' no existe en el servidor."}), 404
            vectordb = FAISS.load_local(faiss_path, embedding, allow_dangerous_deserialization=True)
            vectordbs[especialidad] = vectordb
            print(f"‚úÖ FAISS cargado para {especialidad}.")
        else:
            vectordb = vectordbs[especialidad]

        retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 4})

        # Fusionar fragmentos con RAG Fusion
        documentos = recuperar_fragmentos_fusionados(pregunta_final, retriever)
        contexto = "\n\n".join([doc.page_content for doc in documentos])

        dynamic_prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question", "especialidad_formateada"]
        )

        qa_chain = LLMChain(
            llm=llm,
            prompt=dynamic_prompt.partial(especialidad_formateada=especialidad_formateada)
        )

        result = qa_chain.run({"context": contexto, "question": pregunta_final})
        return jsonify({"respuesta": result})

    except Exception as e:
        logging.error(f"‚ùå Error en /chat/{especialidad}: {str(e)}")
        return jsonify({"error": str(e)}), 500

# üìÑ Endpoint para devolver t√≠tulos de documentos
@app.route("/titulos/<especialidad>", methods=["GET"])
def obtener_titulos(especialidad):
    ruta_txt = os.path.join("data", especialidad, "titulos.txt")
    if not os.path.exists(ruta_txt):
        return jsonify({"error": f"No se encontr√≥ el archivo de t√≠tulos para '{especialidad}'."}), 404

    with open(ruta_txt, "r", encoding="utf-8") as f:
        titulos = [line.strip() for line in f.readlines() if line.strip()]

    return jsonify({"especialidad": especialidad, "titulos": titulos})

if __name__ == "__main__":
    correccion_cache = cargar_cache()
    print(f"üîµ Cache de correcciones cargada ({len(correccion_cache)} entradas).")
    app.run(debug=True)
