import os
import json
import logging
from flask import Flask, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA, LLMChain
from langchain.schema import Document


# üì¶ Cache en memoria
correccion_cache = {}
CACHE_FILE = "cache.json"
PALABRAS_SENSIBLES_FILE = "palabras_sensibles.txt"


# üìù Configurar logging
logging.basicConfig(
    filename="app.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    encoding="utf-8"
)


# üõ° Cargar palabras sensibles desde archivo externo
def cargar_palabras_sensibles():
    if not os.path.exists(PALABRAS_SENSIBLES_FILE):
        mensaje = "‚ö† Advertencia: No se encontr√≥ 'palabras_sensibles.txt'. No se aplicar√° el filtro de protecci√≥n."
        print(mensaje)
        logging.warning(mensaje)
        return []


    with open(PALABRAS_SENSIBLES_FILE, "r", encoding="utf-8") as f:
        lineas = [line.strip().lower() for line in f.readlines() if line.strip()]


    if not lineas:
        mensaje = "‚ö† Advertencia: 'palabras_sensibles.txt' est√° vac√≠o. El filtro de protecci√≥n no detectar√° frases sensibles."
        print(mensaje)
        logging.warning(mensaje)
    else:
        logging.info(f"‚úÖ Se cargaron {len(lineas)} palabras sensibles desde '{PALABRAS_SENSIBLES_FILE}'.")


    return lineas


PALABRAS_SENSIBLES = cargar_palabras_sensibles()


# üì¶ Funciones de cache


def cargar_cache():
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def guardar_cache():
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(correccion_cache, f, indent=2, ensure_ascii=False)


# üå± Cargar entorno
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("‚ùå No se encontr√≥ la API Key. Revisa tu archivo .env.")


# üöÄ Inicializar Flask
app = Flask(__name__)
CORS(app)


# üß† Configurar embeddings y modelos GPT
embedding = OpenAIEmbeddings(
    model="text-embedding-3-large",
    openai_api_key=openai_api_key
)


llm = ChatOpenAI(model="gpt-4o", openai_api_key=openai_api_key, temperature=0)
corrector_llm = ChatOpenAI(model="gpt-4o", openai_api_key=openai_api_key, temperature=0)
parafrasis_llm = ChatOpenAI(model="gpt-4o", openai_api_key=openai_api_key, temperature=0)


# üì¶ FAISS din√°mico por especialidad
vectordbs = {}


prompt_template = """
Est√°s actuando como un asistente especializado en el tema de {especialidad_formateada}, enfocado en responder preguntas de estudiantes de ciencias de la salud, s√© amable y responde con emojis si es apropiado. 


Si te preguntan como te llamas, di que tu nombre es Eini.


Tu conocimiento se basa exclusivamente en libros, decretos, resoluciones y gu√≠as de pr√°ctica cl√≠nica previamente cargadas.


Este chat est√° dedicado √∫nicamente al tema: {especialidad_formateada}. Solo puedes responder con informaci√≥n contenida en los documentos cargados, pero tambi√©n entablar una conversaci√≥n.


No debes inventar informaci√≥n ni dar respuestas fuera de este tema.


Responde exclusivamente usando el contexto proporcionado.


Si te piden t√≠tulos de gu√≠as, resoluciones o decretos, debes extraer literalmente los t√≠tulos encontrados en el contexto.


Formato de respuesta esperado:


RPTA:
[Una respuesta clara, acad√©mica y concisa con base en el contexto. Incluye la fuente si es posible.]


Informaci√≥n adicional:
[Opcional. Puedes incluir otras definiciones, detalles, aclaraciones si el contexto lo permite.]


Fuente:
- [Menciona el n√∫mero de hoja del documento donde se extrajo la respuesta. Enumera URLs, documentos o autores citados. Las fuentes deben aparecer si se han usado. Menciona el t√≠tulo del documento del que fue sacada la informaci√≥n.] 


---


Contexto:
{context}


Pregunta:
{question}
"""



# üßπ Correcci√≥n ortogr√°fica + filtro sensible
def corregir_ortografia_gpt(texto):
    texto_limpio = texto.lower().strip()


    if any(palabra in texto_limpio for palabra in PALABRAS_SENSIBLES):
        mensaje = "‚ö† Pregunta sensible detectada. Derivando a mensaje de apoyo."
        print(mensaje)
        logging.warning(mensaje)
        return (
            "Si est√°s atravesando un momento dif√≠cil, es muy importante que hables con alguien de confianza "
            "o busques apoyo profesional. No est√°s solo/a. Comun√≠cate con una l√≠nea de ayuda en tu pa√≠s o acude a un profesional de salud mental."
        )


    if texto in correccion_cache:
        print("‚ö° Recuperando correcci√≥n de cache (memoria/disco).")
        return correccion_cache[texto]


    prompt_correccion = PromptTemplate(
        input_variables=["texto"],
        template=(
            "Corrige √∫nicamente errores ortogr√°ficos o gramaticales en el siguiente texto en espa√±ol. "
            "No agregues consejos, emociones ni explicaciones. Devuelve el texto corregido, o igual si no hay errores:\n\n"
            "Texto: {texto}\n\nTexto corregido:"
        )
    )


    chain = LLMChain(llm=corrector_llm, prompt=prompt_correccion)
    respuesta = chain.run({"texto": texto}).strip()


    correccion_cache[texto] = respuesta
    guardar_cache()
    print("üíæ Correcci√≥n nueva almacenada en cache y guardada en disco.")
    return respuesta


# üîÅ Par√°frasis inteligente con GPT
def parafrasear_pregunta(texto, tema):
    prompt = PromptTemplate(
        input_variables=["pregunta", "tema"],
        template=(
            "Est√°s reformulando una pregunta de un estudiante sobre el tema de {tema}. "
            "Tu objetivo es que la pregunta sea m√°s clara, directa y espec√≠fica, "
            "pero sin a√±adir informaci√≥n nueva ni extender su contenido. "
            "Solo mejora la redacci√≥n si es necesario. "
            "Si ya es clara, rep√≠tela igual.\n\n"
            "Pregunta original: {pregunta}\n\nPregunta reformulada:"
        )
    )
    chain = LLMChain(llm=parafrasis_llm, prompt=prompt)
    return chain.run({"pregunta": texto, "tema": tema}).strip()


# üîÑ RAG Fusion - combinar resultados de reformulaciones
def recuperar_fragmentos_fusionados(pregunta, retriever):
    reformulaciones = [
        pregunta,
        f"¬øCu√°l es el prop√≥sito de {pregunta}?"
    ]



    documentos = []
    for reformulada in reformulaciones:
        docs = retriever.get_relevant_documents(reformulada)
        documentos.extend(docs)


    # Eliminar duplicados conservando orden
    vistos = set()
    docs_unicos = []
    for doc in documentos:
        if doc.page_content not in vistos:
            vistos.add(doc.page_content)
            docs_unicos.append(doc)


    return docs_unicos


# üö™ Endpoint din√°mico
@app.route("/chat/<especialidad>", methods=["POST"])
def chat_especialidad(especialidad):
    data = request.get_json()
    question = data.get("question", "")


    if not question:
        return jsonify({"error": "Falta la pregunta"}), 400


    try:
        question_corregida = corregir_ortografia_gpt(question)
        especialidad_formateada = especialidad.replace("_", " ").capitalize()
        pregunta_final = parafrasear_pregunta(question_corregida, especialidad_formateada)


        print(f"üîµ Especialidad solicitada: {especialidad}")
        print(f"üîµ Pregunta original: {question}")
        print(f"üîµ Pregunta corregida: {question_corregida}")
        print(f"üîµ Pregunta reformulada: {pregunta_final}")


        if especialidad not in vectordbs:
            faiss_path = f"data/{especialidad}"
            if not os.path.exists(faiss_path):
                return jsonify({"error": f"La especialidad '{especialidad}' no existe en el servidor."}), 404
            vectordb = FAISS.load_local(faiss_path, embedding, allow_dangerous_deserialization=True)
            vectordbs[especialidad] = vectordb
            print(f"‚úÖ FAISS cargado para {especialidad}.")
        else:
            vectordb = vectordbs[especialidad]


        retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 4})


        # Fusionar fragmentos con RAG Fusion
        documentos = recuperar_fragmentos_fusionados(pregunta_final, retriever)
        contexto = "\n\n".join([doc.page_content for doc in documentos])


        dynamic_prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question", "especialidad_formateada"]
        )


        qa_chain = LLMChain(
            llm=llm,
            prompt=dynamic_prompt.partial(especialidad_formateada=especialidad_formateada)
        )


        result = qa_chain.run({"context": contexto, "question": pregunta_final})
        return jsonify({"respuesta": result})


    except Exception as e:
        logging.error(f"‚ùå Error en /chat/{especialidad}: {str(e)}")
        return jsonify({"error": str(e)}), 500


# üìÑ Endpoint para devolver t√≠tulos de documentos
# @app.route("/titulos/<especialidad>", methods=["GET"])
# def obtener_titulos(especialidad):
#     ruta_txt = os.path.join("data", especialidad, "titulos.txt")
#     if not os.path.exists(ruta_txt):
#         return jsonify({"error": f"No se encontr√≥ el archivo de t√≠tulos para '{especialidad}'."}), 404


#     with open(ruta_txt, "r", encoding="utf-8") as f:
#         titulos = [line.strip() for line in f.readlines() if line.strip()]


#     return jsonify({"especialidad": especialidad, "titulos": titulos})


@app.route("/titulos/<especialidad>", methods=["GET"])
def obtener_titulos(especialidad):
    ruta_json = os.path.join("data", especialidad, "titulos.json")
    if not os.path.exists(ruta_json):
        return jsonify({"error": f"No se encontr√≥ el archivo de t√≠tulos para '{especialidad}'."}), 404


    try:
        with open(ruta_json, "r", encoding="utf-8") as f:
            titulos = json.load(f)


        # Validar formato
        if not isinstance(titulos, list) or not all("nombre" in doc and "link" in doc for doc in titulos):
            return jsonify({"error": "Formato inv√°lido en titulos.json"}), 500


        return jsonify({"especialidad": especialidad, "titulos": titulos})
    except Exception as e:
        logging.error(f"‚ùå Error al leer titulos.json para {especialidad}: {str(e)}")
        return jsonify({"error": "No se pudo leer los t√≠tulos"}), 500


if __name__ == "__main__":
    correccion_cache = cargar_cache()
    print(f"üîµ Cache de correcciones cargada ({len(correccion_cache)} entradas).")
    app.run(host="0.0.0.0", port=5000,debug=True)